{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIGITS 64d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 64, 4, 4]             640\n",
      "         LeakyReLU-2             [-1, 64, 4, 4]               0\n",
      "           Dropout-3             [-1, 64, 4, 4]               0\n",
      "            Conv2d-4             [-1, 64, 2, 2]          36,928\n",
      "         LeakyReLU-5             [-1, 64, 2, 2]               0\n",
      "           Dropout-6             [-1, 64, 2, 2]               0\n",
      "           Flatten-7                  [-1, 256]               0\n",
      "            Linear-8                    [-1, 1]             257\n",
      "================================================================\n",
      "Total params: 37,825\n",
      "Trainable params: 37,825\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 0.14\n",
      "Estimated Total Size (MB): 0.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "def loss_nce(r_p, r_q,p_size, q_size):\n",
    "    v = q_size / p_size\n",
    "    return (-(r_p /(v+r_p)).log()).mean() - v* ((v/(v+r_q)).log().mean()) \n",
    "\n",
    "# Load digits data\n",
    "digits = load_digits()\n",
    "X = digits.images\n",
    "y = digits.target\n",
    "\n",
    "# Add Gaussian noise with variance 0.01\n",
    "noise = scs.norm.rvs(0, 0.1, X.shape)\n",
    "X_noisy = (X + noise)/16 # Normalize to [0, 1]\n",
    "\n",
    "# Flatten the images for ECDF transformation\n",
    "X_noisy_flat = X_noisy.reshape(-1, 64)\n",
    "\n",
    "# Apply ECDF transformation\n",
    "X_ecdf = np.zeros_like(X_noisy_flat)\n",
    "ecdf_list = []\n",
    "for dim in range(64):\n",
    "    ecdf = ECDF(X_noisy_flat[:, dim])\n",
    "    ecdf_list.append(ecdf)\n",
    "    X_ecdf[:, dim] = np.clip(ecdf(X_noisy_flat[:, dim]), 1e-6, 1 - 1e-6)\n",
    "\n",
    "# Apply inverse of standard normal CDF (ppf)\n",
    "X_gaussian = scs.norm.ppf(X_ecdf).reshape(-1, 8, 8)\n",
    "y_gaussian = torch.ones(X_gaussian.shape[0], dtype=torch.long)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_gaussian = torch.tensor(X_gaussian, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
    "\n",
    "# Split the data into training and testing sets (50/50 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gaussian, y_gaussian, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create TensorDataset objects\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def reverse_transform(example):\n",
    "    ''' \n",
    "    Reverse the transformation applied to the data using the ECDFs.\n",
    "    \n",
    "    input:\n",
    "        example: torch.Tensor - the transformed example, of shape (1, 8, 8)\n",
    "\n",
    "    output:\n",
    "        original_example: np.array - the original example, of shape (8, 8)\n",
    "    '''\n",
    "    # Convert the tensor to a numpy array and remove the channel dimension\n",
    "    example = example.squeeze().numpy().reshape(-1)\n",
    "    \n",
    "    # Apply the inverse of the standard normal CDF (ppf)\n",
    "    example = scs.norm.cdf(example)\n",
    "    \n",
    "    # Apply the inverse ECDF transformation\n",
    "    original_example = np.zeros_like(example)\n",
    "    for i in range(len(example)):\n",
    "        ecdf = ecdf_list[i]\n",
    "        original_example[i] = np.interp(example[i], ecdf.y, ecdf.x)\n",
    "    \n",
    "    # Reshape back to the original image shape and denormalize\n",
    "    original_example = original_example.reshape(8, 8) * 16\n",
    "    \n",
    "    return original_example\n",
    "\n",
    "\n",
    "# Define the classifier for digits data (8x8 images)\n",
    "class Classifier_Digits(nn.Module):\n",
    "    def __init__(self, in_shape=(1, 8, 8)):\n",
    "        super(Classifier_Digits, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0], 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 2 * 2, 1)  # Adjusted for 8x8 input images\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).exp()\n",
    "\n",
    "# Define model\n",
    "model = Classifier_Digits()\n",
    "\n",
    "# Print model summary\n",
    "summary(model, (1, 8, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 345.6x345.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.matshow(X_noisy_flat[0].reshape(8, 8), cmap='gray')\n",
    "plt.show()\n",
    "plt.matshow(X_ecdf[0].reshape(8, 8), cmap='gray')\n",
    "plt.show()\n",
    "plt.matshow(scs.norm.cdf(X_gaussian[0].reshape(8, 8)), cmap='gray')\n",
    "plt.show()\n",
    "plt.matshow(reverse_transform(X_gaussian[0]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1880034d700>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise_test = torch.randn(100000, 1, 8, 8)\n",
    "r_noise = model(noise_test)\n",
    "idx_sim = np.argmax(r_noise.detach().numpy())\n",
    "plt.matshow(reverse_transform(noise_test[idx_sim].squeeze()), cmap='gray')\n",
    "plt.show()\n",
    "plt.matshow(reverse_transform(noise_test[0].squeeze()), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Loss: 0.28337567646441786\n",
      "Epoch 2/500, Loss: 0.27360423782776144\n",
      "Epoch 3/500, Loss: 0.3806791377478632\n",
      "Epoch 4/500, Loss: 0.25442612787772867\n",
      "Epoch 5/500, Loss: 0.24589012588920264\n",
      "Epoch 6/500, Loss: 0.280538825414561\n",
      "Epoch 7/500, Loss: 0.2559615368473119\n",
      "Epoch 8/500, Loss: 0.26065607381791905\n",
      "Epoch 9/500, Loss: 0.2712930902067957\n",
      "Epoch 10/500, Loss: 0.2601696327328682\n",
      "Epoch 11/500, Loss: 0.315560935386296\n",
      "Epoch 12/500, Loss: 0.24228072320592814\n",
      "Epoch 13/500, Loss: 0.2625442686779746\n",
      "Epoch 14/500, Loss: 0.24811155400399504\n",
      "Epoch 15/500, Loss: 0.2764718625052222\n",
      "Epoch 16/500, Loss: 0.2883329450570304\n",
      "Epoch 17/500, Loss: 0.2537659951084261\n",
      "Epoch 18/500, Loss: 0.23751358238273654\n",
      "Epoch 19/500, Loss: 0.23414004260096058\n",
      "Epoch 20/500, Loss: 0.2385107659702671\n",
      "Epoch 21/500, Loss: 0.274382500299092\n",
      "Epoch 22/500, Loss: 0.29644268821796466\n",
      "Epoch 23/500, Loss: 0.2803063053509285\n",
      "Epoch 24/500, Loss: 0.22574012354016304\n",
      "Epoch 25/500, Loss: 0.2529170821452963\n",
      "Epoch 26/500, Loss: 0.25216780808465233\n",
      "Epoch 27/500, Loss: 0.24992726794604597\n",
      "Epoch 28/500, Loss: 0.2496858996435486\n",
      "Epoch 29/500, Loss: 0.24992927883205743\n",
      "Epoch 30/500, Loss: 0.250059738128992\n",
      "Epoch 31/500, Loss: 0.25302752184456795\n",
      "Epoch 32/500, Loss: 0.24196038677774626\n",
      "Epoch 33/500, Loss: 0.2412614082467967\n",
      "Epoch 34/500, Loss: 0.2544797555190222\n",
      "Epoch 35/500, Loss: 0.2543102844521917\n",
      "Epoch 36/500, Loss: 0.21699940259086675\n",
      "Epoch 37/500, Loss: 0.2599608135120622\n",
      "Epoch 38/500, Loss: 0.2418242588896176\n",
      "Epoch 39/500, Loss: 0.24803903519079604\n",
      "Epoch 40/500, Loss: 0.2384336188692471\n",
      "Epoch 41/500, Loss: 0.22171772862302846\n",
      "Epoch 42/500, Loss: 0.24023485710394793\n",
      "Epoch 43/500, Loss: 0.21191604178527307\n",
      "Epoch 44/500, Loss: 0.22333907079080056\n",
      "Epoch 45/500, Loss: 0.19931811601694288\n",
      "Epoch 46/500, Loss: 0.22018429482805318\n",
      "Epoch 47/500, Loss: 0.2639618179407613\n",
      "Epoch 48/500, Loss: 0.2554098593777624\n",
      "Epoch 49/500, Loss: 0.2182419984440865\n",
      "Epoch 50/500, Loss: 0.21216369022069306\n",
      "Epoch 51/500, Loss: 0.2607170818694707\n",
      "Epoch 52/500, Loss: 0.25065192849985485\n",
      "Epoch 53/500, Loss: 0.2261426092992569\n",
      "Epoch 54/500, Loss: 0.23882451659903445\n",
      "Epoch 55/500, Loss: 0.25954492544305735\n",
      "Epoch 56/500, Loss: 0.20888695300653062\n",
      "Epoch 57/500, Loss: 0.2642974555492401\n",
      "Epoch 58/500, Loss: 0.276067261295072\n",
      "Epoch 59/500, Loss: 0.20871524448538648\n",
      "Epoch 60/500, Loss: 0.28490916973557967\n",
      "Epoch 61/500, Loss: 0.25777940811782046\n",
      "Epoch 62/500, Loss: 0.23859029928029613\n",
      "Epoch 63/500, Loss: 0.22705180920532037\n",
      "Epoch 64/500, Loss: 0.23033070024745217\n",
      "Epoch 65/500, Loss: 0.20932258382953448\n",
      "Epoch 66/500, Loss: 0.20531721583342757\n",
      "Epoch 67/500, Loss: 0.26131981380026914\n",
      "Epoch 68/500, Loss: 0.2667681135494134\n",
      "Epoch 69/500, Loss: 0.20517053917564196\n",
      "Epoch 70/500, Loss: 0.20753492748942867\n",
      "Epoch 71/500, Loss: 0.2201518501187193\n",
      "Epoch 72/500, Loss: 0.20280768741564503\n",
      "Epoch 73/500, Loss: 0.22145464435894172\n",
      "Epoch 74/500, Loss: 0.2146517182860909\n",
      "Epoch 75/500, Loss: 0.2306578431663842\n",
      "Epoch 76/500, Loss: 0.22167966412059192\n",
      "Epoch 77/500, Loss: 0.2273542445538373\n",
      "Epoch 78/500, Loss: 0.21100993184693928\n",
      "Epoch 79/500, Loss: 0.19476705930870156\n",
      "Epoch 80/500, Loss: 0.24914075729661975\n",
      "Epoch 81/500, Loss: 0.25366846048112573\n",
      "Epoch 82/500, Loss: 0.2681693055763327\n",
      "Epoch 83/500, Loss: 0.3415948758865225\n",
      "Epoch 84/500, Loss: 0.2622007005687418\n",
      "Epoch 85/500, Loss: 0.23928914045722322\n",
      "Epoch 86/500, Loss: 0.24767542450592436\n",
      "Epoch 87/500, Loss: 0.23148860011635156\n",
      "Epoch 88/500, Loss: 0.2589691360962802\n",
      "Epoch 89/500, Loss: 0.19395305832912182\n",
      "Epoch 90/500, Loss: 0.23318911722764887\n",
      "Epoch 91/500, Loss: 0.23470910818412385\n",
      "Epoch 92/500, Loss: 0.22659941484091867\n",
      "Epoch 93/500, Loss: 0.2337353671077992\n",
      "Epoch 94/500, Loss: 0.2403731800872704\n",
      "Epoch 95/500, Loss: 0.2476919668762752\n",
      "Epoch 96/500, Loss: 0.24155213632460298\n",
      "Epoch 97/500, Loss: 0.19142196822012292\n",
      "Epoch 98/500, Loss: 0.21801930897194763\n",
      "Epoch 99/500, Loss: 0.2115542872262926\n",
      "Epoch 100/500, Loss: 0.2101155830354526\n",
      "Epoch 101/500, Loss: 0.23927133350536742\n",
      "Epoch 102/500, Loss: 0.24747805862591185\n",
      "Epoch 103/500, Loss: 0.2706158040412541\n",
      "Epoch 104/500, Loss: 0.2088364139199257\n",
      "Epoch 105/500, Loss: 0.19655759709662404\n",
      "Epoch 106/500, Loss: 0.22290903394078387\n",
      "Epoch 107/500, Loss: 0.20238271114970396\n",
      "Epoch 108/500, Loss: 0.21964307023405\n",
      "Epoch 109/500, Loss: 0.24214966209798022\n",
      "Epoch 110/500, Loss: 0.21358864908588343\n",
      "Epoch 111/500, Loss: 0.2125578930617535\n",
      "Epoch 112/500, Loss: 0.2531134474115318\n",
      "Epoch 113/500, Loss: 0.19845157244808748\n",
      "Epoch 114/500, Loss: 0.18442971777604444\n",
      "Epoch 115/500, Loss: 0.2175174699775104\n",
      "Epoch 116/500, Loss: 0.22541246152129665\n",
      "Epoch 117/500, Loss: 0.1916241955397458\n",
      "Epoch 118/500, Loss: 0.2198183629019507\n",
      "Epoch 119/500, Loss: 0.2582809418953698\n",
      "Epoch 120/500, Loss: 0.20301381471277824\n",
      "Epoch 121/500, Loss: 0.22002223529050063\n",
      "Epoch 122/500, Loss: 0.21867295862014952\n",
      "Epoch 123/500, Loss: 0.2345966835474146\n",
      "Epoch 124/500, Loss: 0.21583261854689698\n",
      "Epoch 125/500, Loss: 0.23286414448300313\n",
      "Epoch 126/500, Loss: 0.24021373194610252\n",
      "Epoch 127/500, Loss: 0.19502672928802925\n",
      "Epoch 128/500, Loss: 0.2134595289826393\n",
      "Epoch 129/500, Loss: 0.24847963400955858\n",
      "Epoch 130/500, Loss: 0.22913648393647423\n",
      "Epoch 131/500, Loss: 0.19950873635966201\n",
      "Epoch 132/500, Loss: 0.20094068720936775\n",
      "Epoch 133/500, Loss: 0.1970823440572311\n",
      "Epoch 134/500, Loss: 0.2549251401475791\n",
      "Epoch 135/500, Loss: 0.20835872955523946\n",
      "Epoch 136/500, Loss: 0.2536457797576641\n",
      "Epoch 137/500, Loss: 0.21485574659088563\n",
      "Epoch 138/500, Loss: 0.18002221784714995\n",
      "Epoch 139/500, Loss: 0.1939205239559042\n",
      "Epoch 140/500, Loss: 0.20792557838661918\n",
      "Epoch 141/500, Loss: 0.20861821082131615\n",
      "Epoch 142/500, Loss: 0.20796133070413408\n",
      "Epoch 143/500, Loss: 0.191505899781297\n",
      "Epoch 144/500, Loss: 0.20708889315097495\n",
      "Epoch 145/500, Loss: 0.18801272686185508\n",
      "Epoch 146/500, Loss: 0.17702376572736378\n",
      "Epoch 147/500, Loss: 0.2354589585086395\n",
      "Epoch 148/500, Loss: 0.22394715056850992\n",
      "Epoch 149/500, Loss: 0.22435939106447944\n",
      "Epoch 150/500, Loss: 0.21267499569161186\n",
      "Epoch 151/500, Loss: 0.24137053356088442\n",
      "Epoch 152/500, Loss: 0.17000904268231884\n",
      "Epoch 153/500, Loss: 0.21190441631037613\n",
      "Epoch 154/500, Loss: 0.2076174995243742\n",
      "Epoch 155/500, Loss: 0.18541277177101964\n",
      "Epoch 156/500, Loss: 0.22059236081509753\n",
      "Epoch 157/500, Loss: 0.22678882654370933\n",
      "Epoch 158/500, Loss: 0.18124281281027302\n",
      "Epoch 159/500, Loss: 0.18595305393482076\n",
      "Epoch 160/500, Loss: 0.19070224334678515\n",
      "Epoch 161/500, Loss: 0.21825394516104255\n",
      "Epoch 162/500, Loss: 0.2029290237817271\n",
      "Epoch 163/500, Loss: 0.17120928387960482\n",
      "Epoch 164/500, Loss: 0.20357697996599922\n",
      "Epoch 165/500, Loss: 0.19596059132238913\n",
      "Epoch 166/500, Loss: 0.27575086044340297\n",
      "Epoch 167/500, Loss: 0.21892032052936225\n",
      "Epoch 168/500, Loss: 0.19742776691142855\n",
      "Epoch 169/500, Loss: 0.20146691699608646\n",
      "Epoch 170/500, Loss: 0.20847568481132903\n",
      "Epoch 171/500, Loss: 0.20992224811223045\n",
      "Epoch 172/500, Loss: 0.2276042710732797\n",
      "Epoch 173/500, Loss: 0.22871369919900236\n",
      "Epoch 174/500, Loss: 0.20554346668309179\n",
      "Epoch 175/500, Loss: 0.20146682293250642\n",
      "Epoch 176/500, Loss: 0.21858303473684296\n",
      "Epoch 177/500, Loss: 0.1955670696394197\n",
      "Epoch 178/500, Loss: 0.19702411263153471\n",
      "Epoch 179/500, Loss: 0.1748691361461734\n",
      "Epoch 180/500, Loss: 0.20600252698079266\n",
      "Epoch 181/500, Loss: 0.1953267640080945\n",
      "Epoch 182/500, Loss: 0.19823222376149277\n",
      "Epoch 183/500, Loss: 0.22853711684202327\n",
      "Epoch 184/500, Loss: 0.1900817377749702\n",
      "Epoch 185/500, Loss: 0.20474767941853095\n",
      "Epoch 186/500, Loss: 0.19712116806928454\n",
      "Epoch 187/500, Loss: 0.1733925067659082\n",
      "Epoch 188/500, Loss: 0.1970097592485876\n",
      "Epoch 189/500, Loss: 0.20239593718072463\n",
      "Epoch 190/500, Loss: 0.16483171732197033\n",
      "Epoch 191/500, Loss: 0.1981314430976736\n",
      "Epoch 192/500, Loss: 0.19653570870387144\n",
      "Epoch 193/500, Loss: 0.18819050168104726\n",
      "Epoch 194/500, Loss: 0.23521867057645757\n",
      "Epoch 195/500, Loss: 0.19781359760411854\n",
      "Epoch 196/500, Loss: 0.18974884824249252\n",
      "Epoch 197/500, Loss: 0.2442459772373068\n",
      "Epoch 198/500, Loss: 0.1898384339593608\n",
      "Epoch 199/500, Loss: 0.22019445396767096\n",
      "Epoch 200/500, Loss: 0.18484738674657097\n",
      "Epoch 201/500, Loss: 0.22199979026255937\n",
      "Epoch 202/500, Loss: 0.19419456286163167\n",
      "Epoch 203/500, Loss: 0.1877329851789721\n",
      "Epoch 204/500, Loss: 0.2254773729705605\n",
      "Epoch 205/500, Loss: 0.19517493659052357\n",
      "Epoch 206/500, Loss: 0.20796071917846284\n",
      "Epoch 207/500, Loss: 0.18582536421459298\n",
      "Epoch 208/500, Loss: 0.21156260319824877\n",
      "Epoch 209/500, Loss: 0.19256229004983244\n",
      "Epoch 210/500, Loss: 0.1947249629823812\n",
      "Epoch 211/500, Loss: 0.28106000762561273\n",
      "Epoch 212/500, Loss: 0.20337861545127014\n",
      "Epoch 213/500, Loss: 0.18925569206476212\n",
      "Epoch 214/500, Loss: 0.28598372150084067\n",
      "Epoch 215/500, Loss: 0.22361932206770468\n",
      "Epoch 216/500, Loss: 0.1953158946386699\n",
      "Epoch 217/500, Loss: 0.217866198256098\n",
      "Epoch 218/500, Loss: 0.22113594904156594\n",
      "Epoch 219/500, Loss: 0.21097187966044093\n",
      "Epoch 220/500, Loss: 0.2024053793464755\n",
      "Epoch 221/500, Loss: 0.17760319776576142\n",
      "Epoch 222/500, Loss: 0.17886846024414588\n",
      "Epoch 223/500, Loss: 0.2050823165938772\n",
      "Epoch 224/500, Loss: 0.1960587583739182\n",
      "Epoch 225/500, Loss: 0.18563340912605153\n",
      "Epoch 226/500, Loss: 0.195923317095329\n",
      "Epoch 227/500, Loss: 0.18767824743328423\n",
      "Epoch 228/500, Loss: 0.17782688526244\n",
      "Epoch 229/500, Loss: 0.19279360231654397\n",
      "Epoch 230/500, Loss: 0.19947242634049778\n",
      "Epoch 231/500, Loss: 0.1741470659858194\n",
      "Epoch 232/500, Loss: 0.1613319256696208\n",
      "Epoch 233/500, Loss: 0.18632239822683663\n",
      "Epoch 234/500, Loss: 0.2410396484593893\n",
      "Epoch 235/500, Loss: 0.19559605652466416\n",
      "Epoch 236/500, Loss: 0.15874793248829142\n",
      "Epoch 237/500, Loss: 0.15939971001754547\n",
      "Epoch 238/500, Loss: 0.19451387592687688\n",
      "Epoch 239/500, Loss: 0.18265085615988436\n",
      "Epoch 240/500, Loss: 0.18543356946862205\n",
      "Epoch 241/500, Loss: 0.1719813546615428\n",
      "Epoch 242/500, Loss: 0.17239145382211127\n",
      "Epoch 243/500, Loss: 0.1725865689523775\n",
      "Epoch 244/500, Loss: 0.1706375221497026\n",
      "Epoch 245/500, Loss: 0.18987780385490122\n",
      "Epoch 246/500, Loss: 0.1551134139700825\n",
      "Epoch 247/500, Loss: 0.24440054081637283\n",
      "Epoch 248/500, Loss: 0.17853719908101806\n",
      "Epoch 249/500, Loss: 0.1993020004592836\n",
      "Epoch 250/500, Loss: 0.19447607161669894\n",
      "Epoch 251/500, Loss: 0.20608422731787995\n",
      "Epoch 252/500, Loss: 0.19614638362465234\n",
      "Epoch 253/500, Loss: 0.17035174003705897\n",
      "Epoch 254/500, Loss: 0.18669034929239545\n",
      "Epoch 255/500, Loss: 0.2070610004252401\n",
      "Epoch 256/500, Loss: 0.19661016633798337\n",
      "Epoch 257/500, Loss: 0.17867860409977107\n",
      "Epoch 258/500, Loss: 0.15726035123626733\n",
      "Epoch 259/500, Loss: 0.15132298860056648\n",
      "Epoch 260/500, Loss: 0.17672442053926402\n",
      "Epoch 261/500, Loss: 0.24053410826058225\n",
      "Epoch 262/500, Loss: 0.19352268928597713\n",
      "Epoch 263/500, Loss: 0.20194557424763154\n",
      "Epoch 264/500, Loss: 0.21515663276458608\n",
      "Epoch 265/500, Loss: 0.19364717422888197\n",
      "Epoch 266/500, Loss: 0.18780130957221164\n",
      "Epoch 267/500, Loss: 0.15701922902773166\n",
      "Epoch 268/500, Loss: 0.20187537731795474\n",
      "Epoch 269/500, Loss: 0.1873253509402275\n",
      "Epoch 270/500, Loss: 0.1738080706061988\n",
      "Epoch 271/500, Loss: 0.21432831395288993\n",
      "Epoch 272/500, Loss: 0.22556571739500966\n",
      "Epoch 273/500, Loss: 0.18337734056444005\n",
      "Epoch 274/500, Loss: 0.19417567621788073\n",
      "Epoch 275/500, Loss: 0.1941551625214774\n",
      "Epoch 276/500, Loss: 0.18540411095680862\n",
      "Epoch 277/500, Loss: 0.16918454267855348\n",
      "Epoch 278/500, Loss: 0.18589079620894686\n",
      "Epoch 279/500, Loss: 0.24814457934478235\n",
      "Epoch 280/500, Loss: 0.16248958121085988\n",
      "Epoch 281/500, Loss: 0.16021120068135447\n",
      "Epoch 282/500, Loss: 0.162933939939429\n",
      "Epoch 283/500, Loss: 0.16733427210871515\n",
      "Epoch 284/500, Loss: 0.19118164107203484\n",
      "Epoch 285/500, Loss: 0.2243851949942523\n",
      "Epoch 286/500, Loss: 0.19469875251424723\n",
      "Epoch 287/500, Loss: 0.1789221149580232\n",
      "Epoch 288/500, Loss: 0.1973777952893027\n",
      "Epoch 289/500, Loss: 0.19419470673491215\n",
      "Epoch 290/500, Loss: 0.16686798914753156\n",
      "Epoch 291/500, Loss: 0.1710566078023664\n",
      "Epoch 292/500, Loss: 0.17272255890842142\n",
      "Epoch 293/500, Loss: 0.19052467790657077\n",
      "Epoch 294/500, Loss: 0.15218177052407428\n",
      "Epoch 295/500, Loss: 0.17853140400658393\n",
      "Epoch 296/500, Loss: 0.22044665649019438\n",
      "Epoch 297/500, Loss: 0.17186251645988046\n",
      "Epoch 298/500, Loss: 0.18236394239396886\n",
      "Epoch 299/500, Loss: 0.1738658084951598\n",
      "Epoch 300/500, Loss: 0.17670756038920632\n",
      "Epoch 301/500, Loss: 0.16467655848326354\n",
      "Epoch 302/500, Loss: 0.1974313991336987\n",
      "Epoch 303/500, Loss: 0.18395664363071837\n",
      "Epoch 304/500, Loss: 0.16400959130761952\n",
      "Epoch 305/500, Loss: 0.18130267141708012\n",
      "Epoch 306/500, Loss: 0.18283979286407603\n",
      "Epoch 307/500, Loss: 0.1856566238506087\n",
      "Epoch 308/500, Loss: 0.20519811300368146\n",
      "Epoch 309/500, Loss: 0.17859185792120366\n",
      "Epoch 310/500, Loss: 0.19619151204824448\n",
      "Epoch 311/500, Loss: 0.162090003602849\n",
      "Epoch 312/500, Loss: 0.15969267301261425\n",
      "Epoch 313/500, Loss: 0.198217760996315\n",
      "Epoch 314/500, Loss: 0.14175637852569142\n",
      "Epoch 315/500, Loss: 0.14393882042375103\n",
      "Epoch 316/500, Loss: 0.19507916156074095\n",
      "Epoch 317/500, Loss: 0.19428418152805033\n",
      "Epoch 318/500, Loss: 0.17260470960674615\n",
      "Epoch 319/500, Loss: 0.17909365256541762\n",
      "Epoch 320/500, Loss: 0.16215692192581\n",
      "Epoch 321/500, Loss: 0.2288021400313953\n",
      "Epoch 322/500, Loss: 0.18636895382198795\n",
      "Epoch 323/500, Loss: 0.22262665680770216\n",
      "Epoch 324/500, Loss: 0.18364662815142294\n",
      "Epoch 325/500, Loss: 0.16661512530569372\n",
      "Epoch 326/500, Loss: 0.212133032109203\n",
      "Epoch 327/500, Loss: 0.21463130761323304\n",
      "Epoch 328/500, Loss: 0.19105450308014607\n",
      "Epoch 329/500, Loss: 0.1894523560486991\n",
      "Epoch 330/500, Loss: 0.175111193852178\n",
      "Epoch 331/500, Loss: 0.17575207076452928\n",
      "Epoch 332/500, Loss: 0.14896928763081288\n",
      "Epoch 333/500, Loss: 0.17655993869592404\n",
      "Epoch 334/500, Loss: 0.18366086547230853\n",
      "Epoch 335/500, Loss: 0.1697335986356283\n",
      "Epoch 336/500, Loss: 0.12682072760473037\n",
      "Epoch 337/500, Loss: 0.16679086510477395\n",
      "Epoch 338/500, Loss: 0.2158426146568923\n",
      "Epoch 339/500, Loss: 0.17312603389266237\n",
      "Epoch 340/500, Loss: 0.1569744229059795\n",
      "Epoch 341/500, Loss: 0.19476814735038528\n",
      "Epoch 342/500, Loss: 0.1681218277020701\n",
      "Epoch 343/500, Loss: 0.2022501016999113\n",
      "Epoch 344/500, Loss: 0.18699111275631805\n",
      "Epoch 345/500, Loss: 0.18416543317766026\n",
      "Epoch 346/500, Loss: 0.17636672751014604\n",
      "Epoch 347/500, Loss: 0.16187684096652885\n",
      "Epoch 348/500, Loss: 0.1689701347515501\n",
      "Epoch 349/500, Loss: 0.1721867238139284\n",
      "Epoch 350/500, Loss: 0.19452841361535006\n",
      "Epoch 351/500, Loss: 0.18084107975250688\n",
      "Epoch 352/500, Loss: 0.17072692272606596\n",
      "Epoch 353/500, Loss: 0.1623849944564803\n",
      "Epoch 354/500, Loss: 0.15228558646450782\n",
      "Epoch 355/500, Loss: 0.18462487624893928\n",
      "Epoch 356/500, Loss: 0.16480839535080152\n",
      "Epoch 357/500, Loss: 0.1901431867274745\n",
      "Epoch 358/500, Loss: 0.15785723719103584\n",
      "Epoch 359/500, Loss: 0.2076861896905406\n",
      "Epoch 360/500, Loss: 0.21368934359032146\n",
      "Epoch 361/500, Loss: 0.18282976438259257\n",
      "Epoch 362/500, Loss: 0.19435289148883572\n",
      "Epoch 363/500, Loss: 0.20197184409560828\n",
      "Epoch 364/500, Loss: 0.18646698156050567\n",
      "Epoch 365/500, Loss: 0.18587823687442417\n",
      "Epoch 366/500, Loss: 0.18152894246680984\n",
      "Epoch 367/500, Loss: 0.13527838712365464\n",
      "Epoch 368/500, Loss: 0.15620583026059742\n",
      "Epoch 369/500, Loss: 0.17429530742610322\n",
      "Epoch 370/500, Loss: 0.1449041722406601\n",
      "Epoch 371/500, Loss: 0.17185386270284653\n",
      "Epoch 372/500, Loss: 0.1862244860365473\n",
      "Epoch 373/500, Loss: 0.19064361987442807\n",
      "Epoch 374/500, Loss: 0.17751344192596474\n",
      "Epoch 375/500, Loss: 0.17763528369110207\n",
      "Epoch 376/500, Loss: 0.17575670573218116\n",
      "Epoch 377/500, Loss: 0.1673382891920106\n",
      "Epoch 378/500, Loss: 0.16328444203426098\n",
      "Epoch 379/500, Loss: 0.15497766435146332\n",
      "Epoch 380/500, Loss: 0.1616356449265932\n",
      "Epoch 381/500, Loss: 0.21249198476816045\n",
      "Epoch 382/500, Loss: 0.1362769821466043\n",
      "Epoch 383/500, Loss: 0.19992728936004228\n",
      "Epoch 384/500, Loss: 0.14880950170858154\n",
      "Epoch 385/500, Loss: 0.156134313566546\n",
      "Epoch 386/500, Loss: 0.16769960037721643\n",
      "Epoch 387/500, Loss: 0.15960768764388972\n",
      "Epoch 388/500, Loss: 0.16163936199556136\n",
      "Epoch 389/500, Loss: 0.16915511466755434\n",
      "Epoch 390/500, Loss: 0.1951599419116974\n",
      "Epoch 391/500, Loss: 0.14831871282437753\n",
      "Epoch 392/500, Loss: 0.163608453108062\n",
      "Epoch 393/500, Loss: 0.13271696582950396\n",
      "Epoch 394/500, Loss: 0.1579121533492259\n",
      "Epoch 395/500, Loss: 0.14873028491575527\n",
      "Epoch 396/500, Loss: 0.22140562894015475\n",
      "Epoch 397/500, Loss: 0.14342673784443016\n",
      "Epoch 398/500, Loss: 0.1740099553275725\n",
      "Epoch 399/500, Loss: 0.16814576908303747\n",
      "Epoch 400/500, Loss: 0.22586351979909272\n",
      "Epoch 401/500, Loss: 0.1677796996102251\n",
      "Epoch 402/500, Loss: 0.15747104700783204\n",
      "Epoch 403/500, Loss: 0.16254226593621846\n",
      "Epoch 404/500, Loss: 0.16162607982626248\n",
      "Epoch 405/500, Loss: 0.1385714526384555\n",
      "Epoch 406/500, Loss: 0.16946084977223955\n",
      "Epoch 407/500, Loss: 0.17376177860745068\n",
      "Epoch 408/500, Loss: 0.19807832880780615\n",
      "Epoch 409/500, Loss: 0.18357552215456963\n",
      "Epoch 410/500, Loss: 0.1723076382587696\n",
      "Epoch 411/500, Loss: 0.1452073895468794\n",
      "Epoch 412/500, Loss: 0.16318099843016987\n",
      "Epoch 413/500, Loss: 0.17767191209412855\n",
      "Epoch 414/500, Loss: 0.151454396545887\n",
      "Epoch 415/500, Loss: 0.19659850043284804\n",
      "Epoch 416/500, Loss: 0.13525304160931886\n",
      "Epoch 417/500, Loss: 0.1600058489832385\n",
      "Epoch 418/500, Loss: 0.19463559240102768\n",
      "Epoch 419/500, Loss: 0.1672007146668781\n",
      "Epoch 420/500, Loss: 0.14609175316732506\n",
      "Epoch 421/500, Loss: 0.13809314562842764\n",
      "Epoch 422/500, Loss: 0.18346070132121958\n",
      "Epoch 423/500, Loss: 0.16513154732769933\n",
      "Epoch 424/500, Loss: 0.16646958842616658\n",
      "Epoch 425/500, Loss: 0.21051647809558902\n",
      "Epoch 426/500, Loss: 0.19292259569568881\n",
      "Epoch 427/500, Loss: 0.13491750637005115\n",
      "Epoch 428/500, Loss: 0.1788824904902742\n",
      "Epoch 429/500, Loss: 0.15171392512475623\n",
      "Epoch 430/500, Loss: 0.16726367946328788\n",
      "Epoch 431/500, Loss: 0.17934879994597927\n",
      "Epoch 432/500, Loss: 0.1581479312916254\n",
      "Epoch 433/500, Loss: 0.15912847577369418\n",
      "Epoch 434/500, Loss: 0.12472099441521127\n",
      "Epoch 435/500, Loss: 0.16960101125055346\n",
      "Epoch 436/500, Loss: 0.16842006545128493\n",
      "Epoch 437/500, Loss: 0.14436248850462766\n",
      "Epoch 438/500, Loss: 0.14477978924546261\n",
      "Epoch 439/500, Loss: 0.1710260918941991\n",
      "Epoch 440/500, Loss: 0.14579062383814617\n",
      "Epoch 441/500, Loss: 0.15484859685188737\n",
      "Epoch 442/500, Loss: 0.1818954405085794\n",
      "Epoch 443/500, Loss: 0.1612558366152747\n",
      "Epoch 444/500, Loss: 0.17064807191491127\n",
      "Epoch 445/500, Loss: 0.15992952549252018\n",
      "Epoch 446/500, Loss: 0.17583776824176311\n",
      "Epoch 447/500, Loss: 0.2227044431813832\n",
      "Epoch 448/500, Loss: 0.18163822552767292\n",
      "Epoch 449/500, Loss: 0.16809767253440003\n",
      "Epoch 450/500, Loss: 0.18245479541605916\n",
      "Epoch 451/500, Loss: 0.18635748339624242\n",
      "Epoch 452/500, Loss: 0.1815574204244105\n",
      "Epoch 453/500, Loss: 0.14976057456806302\n",
      "Epoch 454/500, Loss: 0.14207516372974577\n",
      "Epoch 455/500, Loss: 0.18087158544824042\n",
      "Epoch 456/500, Loss: 0.19753667866361552\n",
      "Epoch 457/500, Loss: 0.17593857829041523\n",
      "Epoch 458/500, Loss: 0.1594234365841438\n",
      "Epoch 459/500, Loss: 0.1721562601882836\n",
      "Epoch 460/500, Loss: 0.1543845895613576\n",
      "Epoch 461/500, Loss: 0.14585886508676\n",
      "Epoch 462/500, Loss: 0.1901049380158556\n",
      "Epoch 463/500, Loss: 0.18201387317144666\n",
      "Epoch 464/500, Loss: 0.17208517167982162\n",
      "Epoch 465/500, Loss: 0.15787974578340475\n",
      "Epoch 466/500, Loss: 0.16389334060508629\n",
      "Epoch 467/500, Loss: 0.17460544311023993\n",
      "Epoch 468/500, Loss: 0.1618999509975828\n",
      "Epoch 469/500, Loss: 0.1742691052756433\n",
      "Epoch 470/500, Loss: 0.15718198336403946\n",
      "Epoch 471/500, Loss: 0.17543379230232076\n",
      "Epoch 472/500, Loss: 0.16735372883428273\n",
      "Epoch 473/500, Loss: 0.1644575713635904\n",
      "Epoch 474/500, Loss: 0.17295618124049286\n",
      "Epoch 475/500, Loss: 0.13832165666952215\n",
      "Epoch 476/500, Loss: 0.13720618034231252\n",
      "Epoch 477/500, Loss: 0.1533381327472884\n",
      "Epoch 478/500, Loss: 0.17832770256389832\n",
      "Epoch 479/500, Loss: 0.18648341693112563\n",
      "Epoch 480/500, Loss: 0.1762332428118278\n",
      "Epoch 481/500, Loss: 0.1706679326979893\n",
      "Epoch 482/500, Loss: 0.18599588892840105\n",
      "Epoch 483/500, Loss: 0.14486686782590275\n",
      "Epoch 484/500, Loss: 0.1759013443671424\n",
      "Epoch 485/500, Loss: 0.17305544060494366\n",
      "Epoch 486/500, Loss: 0.16427493904685153\n",
      "Epoch 487/500, Loss: 0.12490397646766284\n",
      "Epoch 488/500, Loss: 0.16001680917267141\n",
      "Epoch 489/500, Loss: 0.1737830146890262\n",
      "Epoch 490/500, Loss: 0.134855756471897\n",
      "Epoch 491/500, Loss: 0.1479426951886251\n",
      "Epoch 492/500, Loss: 0.15677671892375783\n",
      "Epoch 493/500, Loss: 0.15755427975592942\n",
      "Epoch 494/500, Loss: 0.16121511341168962\n",
      "Epoch 495/500, Loss: 0.16449177072479806\n",
      "Epoch 496/500, Loss: 0.1569680518888194\n",
      "Epoch 497/500, Loss: 0.21587715567699794\n",
      "Epoch 498/500, Loss: 0.16575451888914766\n",
      "Epoch 499/500, Loss: 0.1601334174107439\n",
      "Epoch 500/500, Loss: 0.15995713914262838\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        r_p = model(inputs).squeeze()  \n",
    "        r_q = model(torch.randn((10*inputs.shape[0],1,8,8))).squeeze()      \n",
    "        loss = loss_nce(r_p, r_q,inputs.shape[0], 10*inputs.shape[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[220], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m n_runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     43\u001b[0m n_sims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 44\u001b[0m samples, log_pdf_samples \u001b[38;5;241m=\u001b[39m \u001b[43msample_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhmc_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mburn_in_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbimodal_hmc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(samples\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[220], line 24\u001b[0m, in \u001b[0;36msample_from_model\u001b[1;34m(model, n_samples, hmc_runs, burn_in_n, bimodal_hmc)\u001b[0m\n\u001b[0;32m     22\u001b[0m log_pdf_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hmc_run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hmc_runs):\n\u001b[1;32m---> 24\u001b[0m     start_x \u001b[38;5;241m=\u001b[39m \u001b[43msample_ratio_IS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bimodal_hmc:\n\u001b[0;32m     26\u001b[0m         samples1 \u001b[38;5;241m=\u001b[39m hmc(log_r_times_gauss, x0\u001b[38;5;241m=\u001b[39mstart_x[\u001b[38;5;241m0\u001b[39m], n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(n_samples\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m), return_logp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, n_burn\u001b[38;5;241m=\u001b[39mburn_in_n)\n",
      "Cell \u001b[1;32mIn[220], line 16\u001b[0m, in \u001b[0;36msample_from_model.<locals>.sample_ratio_IS\u001b[1;34m(model, n_prop, n_out)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_ratio_IS\u001b[39m(model, n_prop, n_out):\n\u001b[0;32m     15\u001b[0m     z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(n_prop, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     r_log \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     17\u001b[0m             torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlog_prob(torch\u001b[38;5;241m.\u001b[39mtensor(z)\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     18\u001b[0m     sims \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(np\u001b[38;5;241m.\u001b[39marange(n_prop), size\u001b[38;5;241m=\u001b[39mn_out, p\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(r_log \u001b[38;5;241m-\u001b[39m logsumexp(r_log))\u001b[38;5;241m.\u001b[39mflatten(), replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z[sims]\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pyhmc import hmc\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "def sample_from_model(model, n_samples, hmc_runs=5, burn_in_n=1000, bimodal_hmc=False):\n",
    "    def log_r_times_gauss(x):\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "        fun = torch.log(model(x_tensor)).sum() + torch.distributions.Normal(0, 1).log_prob(x_tensor).sum()\n",
    "        fun.backward()\n",
    "        grad_wrt_x = x_tensor.grad\n",
    "        return np.array(fun.item(), dtype=np.float64), np.array(grad_wrt_x.detach().numpy(), dtype=np.float64)\n",
    "\n",
    "    def sample_ratio_IS(model, n_prop, n_out):\n",
    "        z = np.random.randn(n_prop, 1, 8, 8)\n",
    "        r_log = np.log(model(torch.tensor(z).float()).detach().numpy()).sum(axis=(1, 2, 3)) + \\\n",
    "                torch.distributions.Normal(0, 1).log_prob(torch.tensor(z).float()).sum(axis=(1, 2, 3)).detach().numpy()\n",
    "        sims = np.random.choice(np.arange(n_prop), size=n_out, p=np.exp(r_log - logsumexp(r_log)).flatten(), replace=True)\n",
    "        return z[sims]\n",
    "\n",
    "    samples = []\n",
    "    log_pdf_samples = []\n",
    "    for hmc_run in range(hmc_runs):\n",
    "        start_x = sample_ratio_IS(model, 100, 10)\n",
    "        if bimodal_hmc:\n",
    "            samples1 = hmc(log_r_times_gauss, x0=start_x[0], n_samples=int(n_samples/2), return_logp=True, n_burn=burn_in_n)\n",
    "            samples2 = hmc(log_r_times_gauss, x0=-start_x[0], n_samples=int(n_samples/2), return_logp=True, n_burn=burn_in_n)\n",
    "            samples.append(np.concatenate([samples1[0], samples2[0]]))\n",
    "            log_pdf_samples.append(np.concatenate([samples1[1], samples2[1]]))\n",
    "        else:\n",
    "            samples1 = hmc(log_r_times_gauss, x0=start_x[0], n_samples=n_samples, return_logp=True, n_burn=burn_in_n)\n",
    "            samples.append(samples1[0])\n",
    "            log_pdf_samples.append(samples1[1])\n",
    "\n",
    "        print('run', hmc_run, 'DONE')\n",
    "\n",
    "    samples = np.concatenate(samples)\n",
    "    log_pdf_samples = np.concatenate(log_pdf_samples)\n",
    "    return samples, log_pdf_samples\n",
    "\n",
    "# Example usage\n",
    "n_runs = 1\n",
    "n_sims = 2\n",
    "samples, log_pdf_samples = sample_from_model(model, n_sims, hmc_runs=n_runs, burn_in_n=3, bimodal_hmc=True)\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 784d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 14, 14]             640\n",
      "         LeakyReLU-2           [-1, 64, 14, 14]               0\n",
      "           Dropout-3           [-1, 64, 14, 14]               0\n",
      "            Conv2d-4             [-1, 64, 7, 7]          36,928\n",
      "         LeakyReLU-5             [-1, 64, 7, 7]               0\n",
      "           Dropout-6             [-1, 64, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3136]               0\n",
      "            Linear-8                    [-1, 1]           3,137\n",
      "           Sigmoid-9                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 40,705\n",
      "Trainable params: 40,705\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.38\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.54\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'discriminator_plot.png'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Define a classifier for MNIST images\n",
    "class Classifier_MNIST(nn.Module):\n",
    "    def __init__(self, in_shape=(1, 28, 28)):\n",
    "        super(Classifier_MNIST, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0], 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define model\n",
    "model = Classifier_MNIST()\n",
    "\n",
    "# Print model summary\n",
    "summary(model, (1, 28, 28))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Example input for visualization\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "y = model(x)\n",
    "\n",
    "# Plot the model\n",
    "make_dot(y, params=dict(model.named_parameters())).render(\"discriminator_plot\", format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
